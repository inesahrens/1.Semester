% ===========
% Einleitung
% ===========

\chapter{Introduction: What is model order reduction}
\label{chap:introduction}

\begin{itemize}
	\item rigorous definition is difficult/ impossible
	\item MOR is the approximation of large-scale systems of equations by small scale ones with similar solution properties
\end{itemize}

Here, we consider the MOR of dynamical systems of the form
\begin{align*}
	& \dot{x}(t) : = f(x(t), u(t), t) \hspace{2ex} x(t_0)=x_0 \hspace{2ex} t \in I \subset \R \hspace{2ex} I=[t_0, t_f] \\
	& y(t)=g(x(t),u(t),t) 
\end{align*}
with 
\begin{align*}
	& x(\dot ) : I \rightarrow \R^n & \text{state (function)}\\
	& u(\dot ) : I \rightarrow \R^m & \text{input (function)}\\
	& y(\dot ) : I \rightarrow \R^p & \text{output (function)}\\
	& f : \R^n \times \R^m \times I  \rightarrow \R^n & \text{dynamics of system}\\
	& g: \R^n \times \R^m \times I \rightarrow \R^p & \text{output map}\\
\end{align*}

\begin{note}
	if u is fixed we can calculate u, then y. In this lecture we only consider systems where the pde is solvable. 
\end{note}

usually the input can be chosen by a system operator in order to influence the system state. The output consists of the system parameters that are servable or that are of interest to the operator.

The task of MOR can be described as 
\begin{itemize}
	\item find functions 
	\begin{align*}
		& \tilde{f} : \R^l \times \R^m \times I  \rightarrow  \R^n \\
		& \tilde{g}: \R^l \times \R^m \times I \rightarrow \R^p
	\end{align*}
	with $l <<n$ and $\tilde{x}_0 \in \R^l$ such that the output $\tilde{y}$ of the small  system 
	\begin{align*}
		& \dot{\tilde{x}}(t) : = f(\tilde{x}(t), u(t), t) \hspace{2ex} \tilde{x}(t_0)=x_0 \\
		& \tilde{y}(t)=g(\tilde{x}(t),u(t),t) 
	\end{align*}
	is very close to $y$ for most (or all admissable) input functions $u:I \rightarrow \R^m$
	\item meaning of very close will become clear later
	\item purpose of MOR is to aid numerical simulation of large scale systems, which is computationally very demanding for large $n$, but cheap for small $l$. 
\end{itemize}

\begin{exa}
	Consider the linear system with $I=(0,\infty]$
	\begin{align*}
		\begin{pmatrix}
			\dot{x}_1(t) \\\dot{x}_2(t) 
		\end{pmatrix}
		= 
		\begin{pmatrix}
			-1 & 0 \\ 0 & -2
		\end{pmatrix}
		\begin{pmatrix}
			x_1(t) \\ x_2(t)
		\end{pmatrix}
		+
		\begin{pmatrix}
			1 \\ \frac{1}{10}
		\end{pmatrix}
		u(t), \hspace{2ex} 
		\begin{pmatrix}
			x_1(0) \\ x_2(0)
		\end{pmatrix}
		= \begin{pmatrix}
			0 \\ 0
		\end{pmatrix}
	\end{align*}
	
	The state variable $x_1(t)$, $x_2(t)$ are mostly decoupled, so using the formular of variation of the constant, we have
	\begin{eqnarray*}
		y(t)=2x_1(t)+x_2(t) & = 2 \int\limits_0^t e^{- (t- \tau)} u(\tau) \diff \tau + \frac{1}{10} \int\limits_0^t e^{-2(t - \tau )} u(\tau) \diff \tau \\
		& \sim 2 \int\limits_0^t e^{-(t-\tau)} u(\tau) \diff \tau 
	\end{eqnarray*}
	The term after $1/10$ can be erased. 
	
	Hence, the smaller System  $\dot{\tilde{x}}(t)= - \tilde{x}(t) +u(t), \hspace{2ex} \tilde{x}(0)=0, \tilde{y}(t)=2 \tilde{x}(t)$ is an approximation of the original one. 
\end{exa}

\begin{exa}
	We consider the temper distribution $T(t, \xi)$ in a beam of length 1. (\dots) in the lecture we calculate this discrete and the solution $u$ is a \glqq Schock \grqq
\end{exa}

\begin{note}
	lecture 2 is missing
\end{note}

\section{?}

\section{?}

\textbf{recap: dynamical Systems }

\begin{align*}
		& x(\dot ) : I \rightarrow \R^n & \text{state (function)}\\
		& u(\dot ) : I \rightarrow \R^m & \text{input (function)}\\
		& y(\dot ) : I \rightarrow \R^p & \text{output (function)}
\end{align*}

LTI system: There are matrices $A,B,C,D$ s.t. 
\begin{align}
	\label{eq:LTI}
	& \dot{x}(t)=A x(t) + B u(t), \hspace{2ex} x(0)=x_0\\
	& y(t)=C \dot{x}(t) + D u(t)
\end{align}


An LTI can be given by
\begin{itemize}
	\item  $A,B,C,D, x_0$. Then $y(t)= C e^{A(t-\tau)} x_0 + \int\limits_0^t C e^{A(t-\tau)}  B u(\tau) \diff \tau + D u(t)$\item impulse response $y_d$. then $y=y_d * u_0$ (only for $x=0$)
	\item transfer function G. then $L(y)(s)=\hat{y}(u)= G(s)\hat{u}(s)=L(u(s))$ (only for $x=0$)
\end{itemize}

the three parametrization can be transformed into one another. 
\begin{align*}
	y_0(t) & = \left\{ \begin{array}{ll}
			C e^{At}B+ \partial (t) D & t \leq 0 \\
			0	& t<0
	\end{array} \right. \\
	G(s) & = C(sT-A)^{-1} B+D \\
	G(s) & = L(y_d)(s)
\end{align*}
For $u(t)=e^{iwt}I_m$ and $x_0=(iwI-A)^{-1}B$ we have $y(t)= \cdots = G(iw)u(t)$ 
So, excitation of a suitably initialized LTI-System \ref{eq:LTI} with an input of frequency $\omega$ leads to an output of frequency $\omega$ only amplified by a (frequency dependent) factor $G( i \omega) $. For that reason the distraction of G to the imaginary axis is called frequency response.

\section{realization} 

The entries of the matrix valued function G must be rational functions. To see this we use

\begin{defi}
	For $A \in \C^{n \times n}$ we define its adjoint $adj(A) \in \C^{n \times n}$ as
	\begin{align*}
		(adj(A))_{ij}= (-1)^{ij} \det A^{ij}
	\end{align*}
	where $A^{ij}$ is the matrix so that the i-th column and the j-th row is removed. 
\end{defi}

\begin{thm}
	For invertible $A \in \C^{n \times n } $ we have 
	\begin{align*}
		A^{-1}= \frac{adj(A)}{\det A}
	\end{align*} 
\end{thm}
\begin{proof}
	apply Cramers rule to $Ax=I$
\end{proof}

Hence, for the transfer function we have 
\begin{align}
\label{eq:N(s)}
	G(s)=C(sI-A)^{-1}B+D= C \frac{adj(sI-A)}{\det (sI-A)} B + D \notag \\
	= \frac{adj(sT-a)B+\det (sI-A) D}{\det (sT-A)}= \frac{N(s)}{d(s)}
\end{align}
where $d(s)=\det(sI-A)$ is a polynomial of degree n and $N(s):= adj(sI-A)B+\det(sT-A)D$ is a $p \limes m$ matrix polynomial. 

Matrix polyonmials are a matrix whose entries are polynomials. 

\begin{defi}
	The set of $p \times m$ matrix-valued rational functions G st $G(s) \in \R^{p \times m}$ for almost all $s \in \R $ is denoted by $\R^{p \times m}(S)$. Moreover $G \in R^{p \times m}(S)$ is called (strictly) proper if \begin{align*}
		\lim\limits_{s \rightarrow \infty} G(s)
	\end{align*}
	exists (or is zero) 
\end{defi}

\begin{align*}
	\R^{p \times m}_p(s):= \{G \in \R^{p \times m}(s)| G \text{ is proper} \} \\
	\R^{p \times m}_{ps}(s):= \{G \in \R^{p \times m}(s)| G \text{ is strict proper} \} 
\end{align*}

Since 
\begin{align*}
	\lim\limits_{s \rightarrow \infty} (sI -A)^{-1} =0
\end{align*}
we have $G(s) \rightarrow D$.

So a transfer function is proper. It's even strictly proper if $D=0$. Thus the degree of the numerator $N(s)$ in \ref{eq:N(s)} is at most n. Also \ref{eq:N(s)} every pole of $G(s)$ must be a zero of a i.e. an eigenvalue of $A$. The converse is not true. 

\begin{exa}
	\begin{align*}
		A=\begin{pmatrix}
		1 & 1 \\ 0  & 1 
		\end{pmatrix}
		, \hspace{2ex}
		B = \begin{pmatrix}
		3 \\ 0
		\end{pmatrix}, \hspace{2ex}
		C=(1,1), \hspace{2ex} D=0 
	\end{align*}
	So,
	\begin{align*}
		G(s)=\frac{3}{s-1}
	\end{align*}
	The sole pole ist at one which is an eigenvalue, but the eigenvalue 2 is not a pole. Also for $A=1, B=3, C=1, D=0$ we have $G(s)=\frac{3}{s-1}$
\end{exa} 

\begin{defi}
	For given $G \in \R^{p \times m}_p(s)$ the quadruple $[A,B,C,D] \in \R^{n \times n} \times \R^{n \times n} \times \R^{p \times n} \times \R^{p \times m}$ is called a realization (of G), if $G(s)=C(sI-A)^{-1}B + D $. A realization is called minimal if n is minimal among all realizations. Two realizations $[A_1,B_1,C-1,D_1]$ and $[A_2,B_2,C_2,D_2]$  are called equivalent if there is an invertible $T \in \R^{n \times n}$ s.t. $[A_2,B_2,C_2,D_2]= [T^{-1}A_1 T, T^{-1}B_1,C_1T,D_1]$
\end{defi}

\begin{thm}
	Equivalent realizations have the same transfer function. 
\end{thm}

\begin{thm}
	Every $G \in \R^{p \times m}_p(s)$ have a realization. 
\end{thm}
