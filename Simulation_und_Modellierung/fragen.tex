\documentclass[]{article}
\usepackage[utf8]{inputenc}
\usepackage{german}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{xkeyval}
\usepackage{amsthm}
\newtheorem{thm}{Theorem}
\newsavebox{\boiboite}
\newcommand{\titre}{Titre}
\newenvironment{boite}[2][]%
{%
	\renewcommand{\titre}{#2}
	\couleurs[#1]
	\begin{lrbox}{\boiboite}%
		\begin{minipage}[!h]{\size}
		}%
		{%
		\end{minipage}
	\end{lrbox}
	\begin{center}
		\begin{tikzpicture}
		\node [mybox] (box){\usebox{\boiboite}};
		\node[fancytitle, right=10pt] at (box.north west) {\titre};
		\end{tikzpicture}
	\end{center}
}

\newcommand{\couleurs}[1][]{%
	\setkeys{boxedtheorem}{#1}
	\tikzstyle{fancytitle} =[draw=\titleboxcolor, rounded corners, fill=\titlebackground,
	text= \titlecolor]
	\tikzstyle{mybox} = [draw=\boxcolor, fill=\background, very thick,
	rectangle, rounded corners, inner sep=10pt, inner ysep=20pt]
}

%opening
\title{Simulation und Modellierung - Fragen}
\author{}

\begin{document}

\maketitle

\section{Spieltheorie}

\subsection*{Was ist eine strategische Normalform? }
Mit der strategischen Normalform kann ich ein Spiel mathematisch darstellen. Dazu die Strategiemenge, also alle Strategien von allen Spielern und die Auszahlungsfunktion angegeben.  

\subsection*{Nutzenmatrix, Auszahlungsfkt, Zeilen/ und Spaltenspieler erklären können}

Die Auszahlungsfkt beschreibt den Nutzen eines Strategiepaares für einen Spieler. Der Nutzen ist dabei eine reelle Zahl und beschreibt, wie sehr sich das Strategiepaar für den Spieler lohnt. Das Strategiepaar ist ein Paar aus einer Strategie von Spieler A und Spieler B.  

Die Nutzenmatrix hat im ij-ten Eintrag den Nutzen der Strategien von Spieler A und B zu dem Strategiepaar ij. 

Der Zeilenspieler ist der Spieler A und der Spaltenspieler der Spieler B? 


\subsection*{zu einem gegebenen Bsp Nutzenmatrix aufstellen können}
Betrachten wir das Gefangenendilemma. Dabei werden Bankräuber A und B verhafet. Falls beide nicht gestehen bekommen sie beide 7 Jahre Gefängnis. Falls beide gestehen je 3 Jahre. Falls einer Kronzeuge wird, dieser 1 Jahr und der andere 9.

Die daraus resultierende Nutzenmatrix lautet:

\begin{align*}
	\begin{pmatrix}
		(-7,-7) & (-1,-9) \\
		(-9,-1) & (-3,-3)
	\end{pmatrix}
\end{align*}

\subsection*{Was ist eine Reaktionsabb? }
Bei der Reaktionsabbildung schaut ein Spieler sich jede Strategie vom Gegner an und wählt seine optimalen Antworten aus. Dieses ist das Bild der Reaktionsabbildung.
Mathematisch: 
\begin{align*}
	r_X:S_{-X} \rightarrow P(S) \\
	y \mapsto \{ \hat{ x }  \in S_X| U_X(\hat{x},y)= \max\limits_{x \in S_X} U_X(x,y) \} 
\end{align*}

\subsection*{Anhand einer gegebenen Bsp-Nutzenmatrix die Begriffe dominante Strategie, gemischte Strategie und Nashgleichgewicht erklären}

Bei dem Gefangenendilemma ist gestehen die dominante Strategie, da gestehen immer die beste Antwort ist, egal, was der Gegner tut.  

Die Dominante Strategie kann man auch gut bei dem Spiel Stein-Schere-Papier-Brunnen erklären. Die Nutzenmatrix hat folgende Form
\begin{align}
	\begin{pmatrix}
		0 & 1 & -1 & -1 \\
		-1 & 0 & 1 & -1 \\
		1 & -1 & 0 & 1 \\
		1 & 1 & -1 & 0 
	\end{pmatrix}
\end{align}
Die Zeile bzw Spalte 1 ist strickt kleiner als die letzte Zeile/Spalte. Deswegen dominiert der Brunnen den Stein. 

Ein Nashgleichgewicht liegt vor, wenn sowohl die Strategie a die optimale Antwort auf Strategie b ist, als auch b die optimale Antwort auf die Strategie a ist. Bei dem Gefangenen Dilemma ist dies, wenn beide Spieler gestehen. Falls Der Spieler A gesteht, dann möchte der Spieler B ebenfalls gestehen (andernfalls würde er 9 Jahre bekommen) und andersrum genauso. 

Gemischte Strategien hat man, wenn man mit einer gewissen Wahrscheinlichkeit eine Strategie wählt z.B. bei Stein-Schere Papier wäre eine gemischte Strategie gleichverteilt zu spielen. 

\section{Gruppenentscheidungen}

\subsection*{Welche Eigenschaften für Relationen haben wir kennengelernt? }
Wir haben folgende Eigenschaften kennengelernt:
 
Reflexiv: jedes Element steht in relation zu sich selbst. 

Transitiv: $xRy \vspace{1ex} yRz \rightarrow xRz$

symmetrisch: Falls $xRy$ dann gilt auch $yRx$

asymmetrisch: Falls $xRy$ so gilt nicht $yRx$

konvex: Je zwei Elemente müssen in Relation zueinander stehen. Also $xRy$ oder $yRx$

\subsection*{Was ist eine Rangabbildung? }

Eine Rangabbildung spiegelt die Meinung eines Wählers wieder. Eine Kandidatenmenge wird auf einen Abschnitt aus den natürlichen Zahlen abgebildet. Eine Rangabbildung muss surjektiv sein, also jeder Rang muss vergeben werden. Dabei gilt: je kleiner der Rang von einem Kandidaten, desto bevorzugter ist der Kandidat.  

\subsection*{Begriffe, wie z.B. externer/ interner Diktator / Einstimmigkeit erklären können}

\subsubsection*{externer Diktator}
Ein Mensch, der kein Wähler ist, bestimmt den Ausgang der Wahl. Mathematisch hat die kollektive Auswahlfkt egal bei welcher Eingabe, die Ausgabe, die der Diktator möchte. $K(\rho_1, \cdots \rho_n)= \rho_E$

\subsubsection*{interner Diktator}
Hier bestimmt ein Wähler, welches Ergebnis bei der Wahl rauskommt. Also Der Ausgang ist immer durch den i-ten Wähler genau festgelegt, egal, was die anderen wählen.$K(\rho_1, \cdots \rho_n)= \rho_i$

\subsubsection*{Einstimmigkeit}
 Ein Kandiat x wird nur vor y gesetzt, falls alle Wähler x besser finden als y. d.h. $x \rho y : \Leftrightarrow \forall i \in \{1, \dots, n\}: x \rho_i y$
 
 Dieses Verfahren ist kaum anwendbar, da sowas eig nie passiert. Außerdem muss das Einstimmigkeitsprinzip keine Relation liefern, da manche Kandidaten keinem Rang zugeordnet werden. 

\subsection*{Was ist das Cordocct Verfahren? Am geg. Bsp erklären}
 In diesem Verfahren vergleicht man immer zwei Kandidaten miteinander. Man nehme zwei Kandidaten und alle Präferenzrelationen der n Wähler und zählt die Vergleiche, die Kandidat A und die Kandidat B gewinnt. Gewinnt A mehr Vergleiche, so ist Kandidat A vor Kandidat B. Dies tut man nun für alle Kandidaten.
 
 Die Formale Definition lautet: $x \rho y  \Leftrightarrow |\{i \in J| x \rho_i y \} | > | \{ i \in J| y \rho_i x \} | $
 
 Das Problem bei dieser Wahl ist, das Zykel entstehen können. Es kann passieren, dass Kandidat A von B vor C vor A ist. Das wollen wir natürlich nicht.
 
 Ein Beispiel ist die demokratische Familie. 

\subsection*{Was ist die Paretobedingung? Unabhängig von irrelevanten Alternativen}


\subsubsection*{Paretobedingung}
Die Paretobedingung besagt, dass falls sich alle Wähler einig sind, jedes mögliche Wahlergebnis erreichbar sein muss.

Formaler: Die kollektive Auswahlfkt $K:P_A^n \rightarrow P_A$ erfüllt die Paretobedingung, wenn für alle  $\rho_i \in P_A, i=1, \dots, n$ mit $\rho = K(\rho_1, \dots, \rho_n)$ und für alle $x,y \in A$ gilt:
\begin{align*}
\left( \forall i \in \{1, \dots, n \} : x \rho_i y \right) \Rightarrow x \rho y \label{Pareto-Bedingung}
\end{align*} 

\subsubsection*{Unabhängigkeit von irrelevanten Alternativen}
 Die unabhängigkeit von irrelevanten Alternativen sagt, dass die Reihenfolge von zwei Kandidaten sollte nicht dadurch geändert werden können, dass Wähler ihre Präferenz bzgl eines dritten Kandidaten verändern. 
 
 Formal:

Eine kollektive Auswahlfunktion $K : P_A^n \to P_A$ erfüllt die \textbf{Unabhängigkeit von irrelevanten Alternativen}, wenn
$\forall \rho_i \rho'_i \in P_A \quad i=1,...,n$ mit $\rho = K(\rho_1, \dots, \rho_n), \rho' = K(\rho'_1, \dots, \rho'_n)$ und für alle $x,y \in A$ gilt:
\begin{equation*}
\left( \forall i\in \{1, \dots, n\} x \rho_i y \right)
\Rightarrow \left( x \rho y \Leftrightarrow x \rho' y \right)
\end{equation*}

\subsection*{Satz von Arrow (Satz III.4), Aussage + Beweis können, also auch Lemma III.5}

\subsubsection*{Satz von Arrow}
Es zeigt sich im Satz von Arrow, dass kein Verfahren, außer der Interne Diktator alle Bedingungen an ein Verfahren erfüllt.

Formal: 

Es sei $A$ mit $|A| > 2$ eine Kantenmenge und $K: P_A^n \to P_A$ eine kollektive Auswahlfunktion, die die Pareto-Bedingung sowie Unabhängigkeit von irrelevanten Alternativen erfüllt, dann gibt es immer einen (internen) Diktator, das heißt es existiert ein $d \in \{1, \dots, n\}$ sodass für alle $(\rho_1, \dots, \rho_n) \in P_A^n$ gilt
\begin{equation*}
\forall (x,y) \in A \times A : x \rho_d y \Rightarrow x \rho y \text{ mit } \rho = K(\rho_1, \dots, \rho_n)
\end{equation*}
\begin{proof}
	Der Beweis gliedert sich in drei Teile:
	\begin{enumerate}
		\item Extremallemma: Falls alle Wähler einen Kandidaten auf Top oder Flop setzen, so ist dieser in der kollektiven Auswahlfkt ebenfalls dort.
		\item Identifikation eines potentiellen Diktators: Es gibt ein Individuum, das die Auswahlfkt kontrolliert
		\item Dieses Individuum ist der Diktator
	\end{enumerate}
\end{proof}

\subsubsection*{Extremalllemma}
Es gelten die Annahmen aus dem Satz von Arrow. Dann gilt für jede Alternative $y$:\\
Wenn jeder Wähler die Alternative $y$ als beste (\textit{\glqq Top\grqq}) oder als letzte (\textit{\glqq Flop\grqq}) Alternative wählt, dann muss die kollektive Auswahlfunktion die Alternative $y$ ebenfalls \textit{\glqq Top\grqq} oder \textit{\glqq Flop\grqq} setzen. \\
\begin{proof}
	Angenommen die Aussage sei falsch, dann würde die kollektive Auswahlfunktion $K$ die Alternative $y$ nicht an eine Extremalstelle setzen. Es gäbe dann also Alternativen $x,z$, sodass $x \rho y \rho z$. \\
	
	Für jedes individuelle Ranking bei dem $x$ vor $z$ ist, setze $z$ nach vorne. Also: 
	
	Bei jedem individuellen Ranking $\rho_i$ bei dem $y$ \glqq Top\grqq{} ist, setzen wir $z$ an zweite Stelle. 
	
	Bei jedem individuellen Ranking $\rho_i$ bei dem $y$ \glqq Flop\grqq{} ist, setzen wir $z$ auf \glqq Top\grqq{}. \\
	
	Aufgrund der Unabhängigkeit irrelevanter Alternativen gilt immer noch  $x \rho y \rho z$. \\
	Es ist jedoch bei jeder individuellen Präferenz $i$ $z$ vor $x$. \\
	Aufgrund der Pareto-Bedingung muss dann auch $z \rho x$ gelten. Dies führt dann jedoch zu $x \rho y \rho z \rho x$. Dies ist ein Widerspruch zur Transitivität von $\rho$ Widerspruch.
\end{proof}


\subsection*{Was ist eine strategische Manipulation, Top Menge, Diktator einer Entscheidungsfkt}

Zunächst braucht man die Definition der sozialen Entscheidungsfkt:

Die soziale Entscheidungsfkt bildet die Menge der Präferenzen von den Wählern auf die Menge der Kandidaten ab. Also wird jede Präferenz aller Wähler genau ein Kandidat zugeordnet. Dieser Kandidat ist der Sieger der Wahl. Formale Definition: 

  Eine Funktion $ e : P_A^n \to A$  heißt \textbf{soziale Entscheidungsfunktion}


\subsubsection*{Strategische Manipulation}
 Die soziale Entscheidungsfkt heißt strategisch manipulierbar, falls ein Wähler der den Kandidaten B vor den Kandidaten A präferiert, statt seiner Präferenz eine andere angeben kann und damit den Kandidaten B erzwingen kann. 
 
 Die soziale Entscheidungsfkt heißt Anreizkompatibel, falls sie nicht manipulierbar ist. 
 
 Die fomale Definition von strategisch Manipulierbar lautet: 

 Eine soziale Entscheidungsfunktion $e : P_A^n \to A$ kann durch den Wähler $i$ \textbf{strategisch manipuliert} werden, falls es Präferenzen $\rho_1, \dots, \rho_n, \rho'_i \in P_A$ gibt mit $\rho_i \neq \rho'_i$, sodass
 \begin{align*}
 b \rho_i a \text{ gilt für }  a = e(\rho_1, \dots, \rho_i, \dots \rho_n) \text{ und } b= e(\rho_1, \dots, \rho'_i, \dots, \rho_n)
 \end{align*}
 
\subsubsection*{Top Menge} 
Eine Topmenge ist nicht, wie der Name scheint, durch eine Eigenschaft der Menge definiert, sondern durch die Relation, die auf einer Menge von Kandidaten gebildet wird. Die ursprüngliche Relation wird geändert, falls ein Element aus der Top-Menge ist und eines nicht: Dann gilt dass das Element aus der Topmenge immer in Relation zu dem aus der nicht Topmenge steht und nicht andersherum. 

Formaler:

Sei $S \subseteq A$ und $\rho \in P_A$. Wir führen eine weitere Präferenzrelation $\rho^S$ folgendermaßen ein:
\begin{itemize}
	\item für $a,b \in S$: $a \rho^S b \Leftrightarrow a \rho b$
	\item für $a,b \notin S$: $a \rho^S b \Leftrightarrow a \rho b$
	\item für $a \in S, b \notin S$: $a \rho^S b$
\end{itemize}
Man kann zeigen, dass $\rho^S$ dadurch eindeutig bestimmt ist.

\subsubsection*{Diktator einer Entscheidungsfkt}
Auch hier ist der Diktator, wie schon vorher definiert. Falls es einen Wähler gibt,dessen Wahl immer das Gesamtergebnis ist, dann ist dieser Wähler der Diktator.

Formal:  

Wähler $i$ heißt \textbf{Diktator in einer sozialen Entscheidungsfunktion}, falls für alle $\rho_1, \dots, \rho_n \in P_A$ gilt, dass
\begin{equation*}
e(\rho_1,\dots, \rho_n) = a
\end{equation*}
wobei $a$ der eindeutig bestimmte Kandidat $a \rho_i b$ für alle $b \neq a$ ist. Die Funktion $e$ heißt \textbf{diktatorisch}, falls $e$ einen Diktator besitzt.

\subsection*{Satz III.8/ Satz III.10/ Satz III.12/ Satz III.13 und Satz III.14 erklären und beweisen können} 

\begin{thm}
		Eine soziale Entscheidungsfunktion ist genau dann monoton, wenn sie anreizkompatibel ist.
\end{thm}
\begin{proof}
	Sei $e$ monoton. Wann immer $e(\rho_1, \dots, \rho_i, \dots, \rho_n) = a$ und $e(\rho_1, \dots, \rho'_i, \dots, \rho_n) = b$ gilt,
	dann ist aufgrund der Monotonie $a \rho_i b $ und $ b \rho'_i a$. DAnn kann es aber auch keine $\rho_1, \dots, \rho_n, \rho'_i \in P_A$,
	sodass $e(\rho_1, \dots, \rho_i, \dots, \rho_n) = a$ und $e(\rho_1, \dots, \rho'_i, \dots, \rho_n) = b$ und $b \rho_i a$ gilt. Also ist keine Manipulation durch $i$ möglich. \\
	Umgekehrt kann man analog zeigen, dass jede Manipulationsmöglichkeit die Monotonie verletzt.
\end{proof}

\begin{thm}[Top-Präferenz] \label{Top_Praeferenz_Lemma}
	Sei $e$ eine anreizkompatible und surjektive Entscheidungsfunktion. Dann gilt für alle $\rho_1, \dots, \rho_n \in P_A$ und für alle $\emptyset \neq S \subseteq A$:
	\[ e(\rho_1^S, \dots, \rho^S_n) \in S \]
\end{thm}
\begin{proof}
	Sei $a \in S$. Da e surjektiv ist, existieren $\rho_1', \dots \rho_n' \in P_A$, sodass $e(\rho_1', \dots \rho_n')=a$. Nun ändere Schrittweise für $i=1, \dots, n $ die Relation $\rho_i'$ zu $\rho_i^S$. Zu keinem Zeitpunkt kann dabei $b \notin S$ als Ergebnis von e entstehen, da e monoton ist.  
\end{proof}

\begin{thm}
	Falls $e$ eine anreizkompatible und surjektive soziale Entscheidungsfunktion ist, dann ist ihre Erweiterung $\mathcal{E}$ eine kollektiven Auswahlfunktion.
\end{thm}
\begin{proof}
	zu zeigen ist: $\rho \in P_A$ \\
	\begin{itemize}
		\item[\it Asymmetrie:] Wegen Lemma \ref{Top_Praeferenz_Lemma} gilt mit $e\left(\rho_1^{\{a, b \}}, \dots, \rho_n^{\{a, b \}} \right) \in \{a,b\}$ entweder $a \rho b$ oder $b \rho a$. 
		\item[\it Transitivität:] Angenommen, $\rho$ wäre nicht transitiv, daas heißt es gelte $a \rho b$ und $b \rho c$ aber nicht $a \rho c$. 
		Wegen der Asymmetrie gilt somit $c \rho a$.\\
		Betrachte die Menge $S := \{a,b,c\}$. Dann sei o.b.d.A.:
		\[ e\left(\rho_1^{\{a, b,c \}}, \dots, \rho_n^{\{a, b, c \}} \right) = c \]
		Aufgrund der Monotonie gilt
		\[  e\left(\rho_1^{\{b, c \}}, \dots, \rho_n^{\{b, c \}} \right) = c \]
		durch schrittweise Änderung von $\rho_i^{\{a,b,c\}}$ auf $\rho_i^{\{b,c\}}$. \\
		Also haben wir dann $c \rho b$ Widerspruch.
	\end{itemize}
\end{proof}

\begin{thm}[Erweiterungslemma]
	Falls $e$ eine anreizkompatible, surjektive und nicht-diktatorische Entscheidungsfunktion ist, so ist ihre Erweiterung $\mathcal{E}$ eine kollektive Auswahlfunktion, die Einstimmigkeit, Unabhängigkeit irrelevanter Alternativen und nicht-diktatorische Entscheidungen erfüllt.
\end{thm}
\begin{proof}~
	\begin{itemize}
		\item[\it Einstimmigkeit:] Sei $a \rho_i b$ für alle $i=1, \dots, n$. Dann ist wegen Lemma \ref{Top_Praeferenz_Lemma} 
		\[e\left(\rho_1^{\{a, b \}}, \dots, \rho_n^{\{a, b \}} \right) = a\] 
		und somit $a \rho b$.
		\item[\it Unabhängigkeit irrelevanter Ereignisse:] Falls für alle $i = 1, \dots, n$ gilt
		\[a \rho_i b \Leftrightarrow a \rho'_i b \]
		dann muss
		\[e\left(\rho_1^{\{a, b \}}, \dots, \rho_n^{\{a, b \}} \right) 
		= 
		e\left(\rho_1^{\prime \{a, b \}}, \dots, \rho_n^{\prime \{a, b \}} \right) \]
		gelten, da sich das Ergebnis wegen der Monotonie von $e$ nicht ändert, wenn man $S_i^{\{a,b\}}$ schrittweise zu $S_i^{\{a,b\}}$ verändert.
		\item[\it Diktator:] Übung.
	\end{itemize}
\end{proof}

\begin{thm}[Satz von Gibbard-Satterthwaite]
	Falls $e$ eine surjektive, anreizkompatible Entscheidungsfunktion ist, so dass drei oder mehr Alternativen wählbar sind, dann ist $e$ diktatorisch.
\end{thm}

\section{Informationssuche im Netz}

\subsection*{Was ist ein Page Rank Vektor, ein Webgraph und eine Hyperlinkmatrix? (auch am geg. Bsp erklären können)}
\subsubsection*{Page Rank Vektor}
Der Page Rank Vektor soll die Wichtigkeit aller Webseiten bzgl. einer Suchanfrage beschreiben. Jeder Eintrag im Vektor gehört zu einer Webseite. Je höher der Wert, desto wichtiger die Seite. Er beschreibt also die Reihenfolge der Webseiten, wie sie ein Nutzer sieht, wenn er eine Suchanfrage stellt. 
Man kann den Page Rank Vektor auf unterschiedliche Arten berechnen.
Die Grundlegende Idee ist es, dass eine Webseite wichtiger ist, je mehr Links auf sie zeigen.  
Wenn man den Page Rank aus einer anderen Perspektive betrachtet, gibt er die Wkeit an, mit der sich ein Nutzer auf einer Webseite befindet. 

\subsubsection*{Webgraph}
Ein Webgraph $G=(S,E)$ beschreibt, die Verlinkung unter allen Webseiten. Dabei ist $S$ die Knotenmenge, also die Menge der Webseiten und $E$ Kantenmenge, also die Menge aller Links. Dabei bedeutet $A \rightarrow B$, dass Webseite A einen Link hat, der auf die Webseite B verweist. 

\subsubsection*{Hyperlinkmatrix}
Die Hyperlinkmatrix beschreibt die Verlinkung der Webseiten untereinander. 
\begin{equation*}
H \in \mathbb{R}^{n \times m} \qquad h_{ij} = 
\begin{cases}
\frac{1}{|S_i|} & S_i \text{ besitzt einen Link auf }S_j \\
0 & \text{sonst}
\end{cases}
\end{equation*}
 $|S_i|$ ist die Anzahl der Hyperlinks, die von der Website $S_i$ auf andere Webseiten verweisen.
 
 Am besten kann man dies an einem Bsp erklären: 

Grafik aus dem Script.

\begin{equation}
H = 
\begin{pmatrix}
0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
0 & 0 & 0 & 1\\
1 & 0 & 0 & 0 \\
\frac{1}{2} & 0 & \frac{1}{2} & 0
\end{pmatrix}
\end{equation}

\subsection*{Was ist ein Zufallssurfer? }
Ein Zufallssurfer ist ein Websurfer, der zufällig auf eine Seite gelangt und sich von dort aus zufällig mit einem bestimmten Schema weiterbewegt z.b. könnte das Schema sein, dass er immer zufällig einen Link auf der Seite wählt. 

\subsection*{Was ist eine Markovkette/ Markovprozess? }
Markov Kette = Markov Prozess

Wir gehen davon aus, dass wir zum k-ten Zeitpunkt einen bestimmten Zustand erreicht haben. Nun wollen wir die Wkeit für ein bestimmtes Ereignis zum nächsten Zeitpkt bestimmen. Falls diese Wkeit nicht davon abhängig ist, welche Ereignisse vor dem k-ten Zeitpunkt eingetreten sind, dann nennen wir den stochastischen Prozess einen Markowprozess.
Formal: 
Sei $t_1 < t_2 < \dots < t_{k+1} \in \mathcal{T}$ eine Folge von Zeitpunkten, dann ist X(t) ein Markovprozess, falls
$ \mathbb{P}[X_{t_{k+1}} = x_{k+1} | X_{t_k} = x_{k}, \dots , X_{t_1} = x_{1} ] = \mathbb{P} [X_{t_{k+1}} = x_{t_{k+1}} | X_{t_k} = x_{t_k} ] $

Wenn wir die Webseiten als Ereignisse und die diskreten Zeitpunkte betrachten ist das wechseln von einer Webseite zu einer anderen ein Markowprozess, da er nicht davon abhängt, welche Seiten der Nutzer vorher besucht hat. Die Links auf einer Seite verändern sich dadurch nicht

\subsection*{Was ist eine Übergangsmatrix/ Markoveigenschaft?}

\subsubsection*{Übergangsmatrix}
Die Übergangsmatrix beschreibt, ähnlich wie die Hyperlinkmatrix, die Verlinkung der Webseiten untereinander. Nur kommt hier ein Term hinzu, der in Kraft tritt, sobald eine Webseite überhaupt keinen Link hat. Dann springt der Zufallssurfer gleichverteilt auf eine beliebige Webseite.
 
Formal:
Sei
$ \mathbb{P}[X_{k+1} = S_j | X_k = S_i ]$  die Wahrscheinlichkeit im $k$-ten Zeitschritt von einer Seite $S_i \in E$ auf $S_j \in E$ zu wechseln. Dieses ist, dadurch dass wir eine Markowkette haben, Unabhängig vom Zeitpunkt $k$. Definiere nun
\begin{align*}
 \mathbb{P} [X_1 = S_j | X_0 = S_i ] =
\begin{cases}
\frac{1}{|S_i|}		& S_i \text{ besitzt einen Link zu } S_j \\
\frac{1}{n}			& S_i \text{ besitzt überhaupt keinen Link} \\
0					& \text{sonst}
\end{cases}
\end{align*}
Damit können wir die \textbf{Übergangsmatrix der Markov-Kette} aufstellen:

\begin{equation*}
U \in \mathbb{R}^{n,m} \qquad u_{ij} := \mathbb{P} [X_1 = S_j | X_0 = S_i ]
\end{equation*}
Die Matrix $U$ ist eine \textbf{stochastische Matrix}, das heißt
\[ u_{ij} \geq 0 \text{ und } \sum_{j=1}^n u_{ij} =1 \]

\subsubsection*{Markov Eigenschaft}
Die Markov Eigenschaft, ist die Eigenschaft einer Markov Kette, dass 
$ \mathbb{P}[X_{t_{k+1}} = x_{k+1} | X_{t_k} = x_{k}, \dots , X_{t_1} = x_{1} ] = \mathbb{P} [X_{t_{k+1}} = x_{t_{k+1}} | X_{t_k} = x_{t_k} ] $

\subsection*{Wie kann man eine Markov Kette für das Google-Problem benutzen? }
Markov Ketten kann man für das Google Problem nutzen, da die Modellierung mit der Übergangsmatrix die Markov Eigenschaft erfüllt. 

Dabei ist nun $\pi_j^{(k)} = \mathbb{P} [X_k = S_j] $
die Wahrscheinlichkeit, dass der Zufallssurfer im $k$-ten Schritt auf $S_j$ ist. 
Mit der Morkov Eigenschaft gilt, dass $\mathbb{P}[X_n=j]= \sum\limits_{i \in I} \mathbb{P}[X_0 = i] \mathbb{P}[X_n=j|x_0=i]$.
Dies kann man benutzen, um zu zeigen, dass  
\begin{equation*}
\left( \pi^{(k)} \right)^T
= \left( \pi^{(k-1)} \right)^T U
= \dotsc 
= \left( \pi^{(0)} \right)^T U^k
\end{equation*}
gilt, wobei U wie vorhin die Übergangsmatrix ist. 

\subsection*{Perron Frobenius Theorie erklären können und die entsprechenden Definitionen beherrschen, sowie Satz IV.1, Satz IV.2, Lemma IV.4, Satz IV.5 erklären und beweisen können}
Die Perron Frobenius Theorie beschäftigt sich mit der Frage, ob $A^T x = x$ eine eindeutige Lösung besitzt. \\

hie am besten erst später eine genaue erklärung einfügen. \\

\subsubsection*{einfacher, strikt dominanter Eigenwert}
$|\lambda_1|=1$ ist der betragsmäßig größte EW und ist eindeutig.

\subsubsection*{Theorem IV.1}

Sei $\lambda_1$ ein einfacher und strikt dominanter Eigenwert der Matrix $A \in \mathbb{C}^{n \times n}$ mit dem zugehörigen (Rechts-)Eigenvektor $v_1$ und Linkeigenvektor $w_1$ mit $w_1^T v_1 = 1$. Dann gilt für jede Lösung von \[ x^{k+1} = A x^k \qquad k=0,1,2,\dotsc \]
dass
\begin{equation*}
\lim_{k \to \infty} \frac{x^k}{\lambda_1^k} = (w_1^T x^0 ) v_1
\end{equation*}

\begin{proof}
	Wir führen den Beweis für diagonalisierbare Matrizen $A$ durch (der allgemeine Beweis ist möglich mit Hilfe der Jordan-Normalform). \\
	Sei also $A$ diagonalisierbar mit Eigenwerten $\lambda_1, \dotsc, \lambda_n$ und $|\lambda_1| > |\lambda_i|$, $i=2, \dots, n$, dann folgt
	\begin{equation*}
	\lim_{k \to \infty} \frac{\lambda_i^k}{\lambda_1^k} = 0 \qquad i=2,3,4, \dotsc
	\end{equation*}
	Zusammen mit \eqref{loesung_x_k_eigenbasis} folgt also
	\begin{equation*}
	\lim_{k \to \infty} \frac{x^k}{\lambda_1^k} = \beta_1 v_1
	\end{equation*}
	Für die (Rechts-)Eigenvektoren $v_1, \dotsc, v_n$ von A gilt
	\begin{equation*}
	w_1^T A v_i = w_1^T \lambda_i v_i = \lambda_1 w_1^T v_i
	\Rightarrow
	(\lambda_i - \lambda_1) w_1^T v_i = 0
	\end{equation*}
	Wegen $\lambda_i - \lambda_1 \neq 0$ folgt $w_1^T v_2 = \dots = w_1^T v_n = 0$ und damit
	\[ w_1^T x^0 = \beta_1 w_1^T v_1 = \beta_1 \]
\end{proof}

\subsubsection*{Positive Matrizen}
Eine Matrix $A$ heißt \textbf{positiv}, falls alle Einträge in $A$ positiv sind. \\
$\sigma(A) = \{ \lambda \in \mathbb{C} | Ker(A - \lambda I) \neq 0 \}$ heißt \textbf{Spektrum} der Matrix $A$. \\
$\rho(A) = \max \{|\lambda| | \lambda \in \sigma(A) \}$ heißt \textbf{Spektralradius} von $A$.

\subsubsection*{Satz von Perron (Satz IV.2)}
Sei $A>0$ mit Spektralradius $\rho(A)=1$, dann gilt
\begin{enumerate}
	\item Der Spektralradius $\rho(A)=1$ ist Eigenwert
	\item Der Eigenwert $\lambda = 1$ ist der einzige auf dem Einheitskreis
	\item Zu $\lambda = 1$ exisiteren positive Rechts- und Linkseigenvektoren
	\item Der Eigenwert $\lambda=1$ hat algebraische Vielfachheit 1
	\item Außer dem Rechtseigenvektor $v_1$ gibt es keine weiteren nicht negativen Eigenvektoren von $A$ (bis auf skalare Vielfache)
\end{enumerate}
\begin{proof}~
	\begin{enumerate}
		\item Betrachte zunächst mögliche Eigenvektoren $x$ zum Eigenwert $\lambda$ auf dem Einheitskreis $|\lambda| = 1$. Für diese gilt
		\begin{equation} \label{proof_perron1}
		|x| = |\lambda| |x| = |\lambda x | = |Ax| \leq |A| |x| = A |x|
		\end{equation} 
		Falls solche Eigenvektoren $x \neq 0$ existieren, so gilt
		\[ z := A |x| > 0 \]
		Definiere $y:=z - |x|$, dann kann man \eqref{proof_perron1} zu
		$y \geq 0$ umformulieren. \\
		Nehmen wir nun an, dass $y \neq 0$ gelten würde, so folgt
		\[ Ay > 0 \]
		Dann existiert auch $\tau>0$, sodass $Ay > \tau z$ womit folgt
		\begin{equation*}
		Ay = Az - A|x| = Az - z > \tau z  \Leftrightarrow Az > (1+\tau) z
		\end{equation*}
		Setze $B := \frac{1}{1+\tau} A$, dann gilt
		\[ Bz > z \]
		Wiederholtes Anwenden ergibt schließlich
		$B^k z > z$ und mit $\rho(B) = \frac{1}{1+\tau} < 1$ folgt
		\[ \lim_{k \to \infty} B^k z = 0 > z \]
		Dies steht im Widerspruch zu $z>0$ bzw. $y \neq 0$ \hfill. Widerspruch \\
		Also ist 
		\[y = A|x| - |x| = 0 \]
		und damit existiert ein Eigenwert $\lambda = 1$
	\item Die Widerspruchsannahme $y \neq 0$ ($A|x| - |x| \neq 0$) schloss $\lambda = 1$ mit ein. Somit ist $\lambda = 1 \in \mathbb{R}$  einziger Eigenwert auf dem Einheitskreis.
	\item Wegen $|x| = A|x| > 0$ sind alle Komponenten positiv. Dies gilt auch für Links- Rechtseigenvektoren gleichermaßen, da auch $A^T$ positiv ist
	\item \textit{Übung}
	\item \textit{Übung}
\end{enumerate}
\end{proof}

\subsubsection*{irreduzibel}
Sei $A \in \mathbb{R}^{n \times n}$. \\
Eine Matrix $P \in \mathbb{R}^{n \times n}$ heißt \textbf{Permutationsmatrix}, falls $p_{ij} \in \{0,1\}$ und jede Spalte und Zeile genau eine Eins enthält. \\
$A$ heißt \textbf{reduzibel}, falls es eine Permutationsmatrix $P$ gibt, sodass
\[ P^T A P = 
\begin{pmatrix}
C & D \\
0 & F 
\end{pmatrix}  \]
wobei die Blockmatrizen $C$ und $F$ quadratisch sein müssen. \\
Falls kein solcher \textit{Nullblock} existiert, so ist die Matrix \textbf{irreduzibel}.

\subsubsection*{Charakterisierung der Irreduzibilität: Lemma IV.4}
Falls $A \in \mathbb{R}^{n \times n}, A \geq 0$ irreduzibel ist, dann gilt
\[ (I + A)^{n-1} > 0 \]
\begin{proof}
	Seien $A^k = (a_{ij}^{(k)} )_{i,j = 1, \dotsc, k}$ die Potenzen der nicht-negativen Matrix $A$. Elementweise gilt dann
	\begin{equation*}
	a_{ij}^{(k)} = \sum_{l_1, \dotsc, l_k} a_{i l_1} \cdot, a_{l_1 l_2} \dots a_{l_{k-1} j}
	\end{equation*}
	Diese Elemente verschwinden, falls mindestens einer der Faktoren verschwindet, also falls beim zugehörigen Graphen kein Pfad vom Knoten $i$ zu $j$ existiert. \\
	Falls jedoch dieser Graph existiert, so ergibt es mindestens eine Indexfolge $a_{i l^*_1} >0, \dotsc, a_{l^*_{k-1} j} >0$. Bei einer irreduziblen Matrix tritt dieser Fall spätestens nach durchlaufen aller Knoten auf, also $n-1$:
	\begin{equation*}
	\left[ (I + A)^{n-1} \right]_{ij} = \left[ \sum_{k=0}^{n-1} \begin{pmatrix} n-1 \\ k \end{pmatrix} A^k \right]_{ij} = \sum_{k=0}^{n-1} \begin{pmatrix} n-1 \\ k \end{pmatrix} a_{ij}^{(k)} > 0
	\end{equation*}
	\end{proof}

\subsubsection*{Satz von Perron Frobenius. Satz IV.5}	
Es sei $A \geq 0$ eine irreduzible stochastische Matrix. Dann gilt:
\begin{enumerate}
	\item Der Perron-Eigenwert $\lambda = 1$ ist einfach
	\item Zu $\lambda=1$ existiert ein Linkseigenvektor $\pi^T > 0$
\end{enumerate}


\begin{proof}
	\begin{enumerate}
		\item
		Sei $A$ stochastisch, also $Ae = e$ mit $e^T = (1, \dotsc, 1)^T$ Rechtseigenvektor zum Eigenwert $\lambda = 1$ ist. Es gilt
		\begin{equation*}
		|\lambda(A)| \leq \rho(A) \leq \Vert A \Vert_\infty \leq 1
		\end{equation*}
		wobei $\Vert \cdot \Vert_\infty$ die Maximumsnorm ist. \\
		Zeige nun: Der Eigenwert ist einfach. Betrachte dazu
		\[ B := (I + A)^{n-1} > 0\]
		Seien nun $\lambda$ die Eigenwerte von $A$, dann sind die Eigenwerte von $B$ gerade {\nolinebreak $(1+\lambda)^{n-1}$}. \\
		\begin{equation} \label{proof_frobenius1}
		\mu = \rho(B) = \max_{|\lambda| < 1} |1+ \lambda|^{n-1} = 2^{n-1}
		\end{equation}
		Dieser Eigenwert ist einfach und liegt auf dem Kreis mit Radius $\mu$. Das Maximum in \eqref{proof_frobenius1} wird für $\lambda = 1$ angenommen. Also sind Multiziplität des Eigenwertes $\mu$ von $B$ und des Eigenwertes $\lambda$ von $A$ gleich.	
\item Jeder Eigenvektor zum Eigenwert $\lambda$ von $A$ ist auch zugleich Eigenvektor zum Eigenwert $(1 + \lambda)^{n-1}$ von $B$.\\
Sei $x$ Eigenvektor zu $\mu$ von $B$ und zugleich $\lambda=1$ zu $A$, dann ist $x = |x| > 0$ (aufgrund des Satzes \ref{Perron}). 
Durch Anwendung auf den Links-Eigenvektor erhält man mit Satz \ref{Perron}c) die Behauptung.
\end{enumerate}
\end{proof}
\subsection*{Erklären können, wie der Page Rank Vektor mittels Vektoriteration berechnet wird}

\subsection*{Konvergenzgeschwindigkeit/ Sensitivitätsanalyse (Satz IV.8 erklären und beweisen können), Satz IV.9/ satz IV.10 erklären und grobe Beweisskizze}

\subsection*{Satz IV.11 und Satz IV.12 erklären und beweisen können}

\section{Verkehrssimulation}

\subsection*{Grundidee der makroskopischen Verkehrssimulation}

\subsection*{Begriffe: Fluss, dichte und Fundermentaldiagramm erklären können}

\subsection*{Erhaltungssätze herleiten können(Anzahl der Autos)}

\subsection*{Modellierung der Geschwindigkeit (über Relativgeschwindigkeit)}

\subsection*{lineare DGL erklären (Gleichung für Störung)}

\subsection*{Charakteristik erklären können und an einem gege Bsp berechnen und skizzieren können}

\subsection*{Numerische Approximation und CFL Bedingung erklären können}

\subsection*{Ungleichförmiger Verkehr/Anfahrvorgang an grüner Ampel/ Anhalten an roter Ampel und unstetige Verkehrsdichte erklären}

\subsection*{Grundidee Godunov Methode}

\subsection*{Grundidee der Mikroskopischen Modellierung mittels Zellulärer Automaten}

\subsection{Stochastische Verkehrssimulation: Wartesystem, exponentielle Verteilung, Poisson Prozess, Ausfallrate erklären und Satz V.1 beweisen können}


\end{document}